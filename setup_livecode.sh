#!/bin/bash
# ============================================================
# setup_livecodebench.sh
# å°† LiveCodeBench æ•´åˆåˆ° OThink-R1 é¡¹ç›®çš„ benchmark/ ç›®å½•ä¸‹
#
# ä½¿ç”¨æ–¹æ³•:
#   cd ~/ACL-ARR-Jan-Rebuttal/OThink-R1   # é¡¹ç›®æ ¹ç›®å½•
#   bash setup_livecodebench.sh [/path/to/local/LiveCodeBench]
#
# å‚æ•°:
#   $1 (å¯é€‰): æœ¬åœ°å·² clone çš„ LiveCodeBench è·¯å¾„
#              å¦‚æœä¸ä¼ ï¼Œåˆ™ä» GitHub clone
# ============================================================
set -e

PROJECT_ROOT="$(cd "$(dirname "$0")" && pwd)"
BENCHMARK_DIR="${PROJECT_ROOT}/benchmark/livecodebench"
LCB_SRC_DIR="${BENCHMARK_DIR}/LiveCodeBench"

echo "=========================================="
echo "  LiveCodeBench â†’ OThink-R1 æ•´åˆ"
echo "  é¡¹ç›®æ ¹ç›®å½•: ${PROJECT_ROOT}"
echo "=========================================="

# ============================================================
# Step 1: åˆ›å»º benchmark ç›®å½•ç»“æ„ + æ‹·è´/clone LiveCodeBench
# ============================================================
echo ""
echo "[1/6] åˆ›å»º benchmark/livecodebench/ ç›®å½•å¹¶å¯¼å…¥ LiveCodeBench æºç ..."

mkdir -p "${BENCHMARK_DIR}"

if [ -n "$1" ] && [ -d "$1" ]; then
    echo "  ä½¿ç”¨æœ¬åœ° LiveCodeBench: $1"
    if [ -d "${LCB_SRC_DIR}" ]; then
        echo "  âš ï¸  ${LCB_SRC_DIR} å·²å­˜åœ¨ï¼Œè·³è¿‡æ‹·è´"
    else
        cp -r "$1" "${LCB_SRC_DIR}"
        echo "  âœ… å·²æ‹·è´åˆ° ${LCB_SRC_DIR}"
    fi
else
    if [ -d "${LCB_SRC_DIR}" ]; then
        echo "  âš ï¸  ${LCB_SRC_DIR} å·²å­˜åœ¨ï¼Œè·³è¿‡ clone"
    else
        echo "  ä» GitHub clone LiveCodeBench..."
        git clone https://github.com/cynthia-shengjia/LiveCodeBench.git "${LCB_SRC_DIR}"
        echo "  âœ… clone å®Œæˆ"
    fi
fi

# ============================================================
# Step 2: æ›´æ–° pyproject.toml æ·»åŠ  LiveCodeBench ä¾èµ–
# ============================================================
echo ""
echo "[2/6] æ£€æŸ¥å¹¶æ›´æ–° pyproject.toml ä¾èµ–..."

PYPROJECT="${PROJECT_ROOT}/pyproject.toml"
DEPS_TO_ADD=("pebble>=5.1.0" "annotated-types>=0.7.0")

for dep in "${DEPS_TO_ADD[@]}"; do
    dep_name=$(echo "$dep" | sed 's/[>=<].*//')
    if grep -qi "\"${dep_name}" "${PYPROJECT}"; then
        echo "  âš ï¸  ${dep_name} å·²åœ¨ pyproject.toml ä¸­"
    else
        # åœ¨ "# DEER Baseline Dependencies" è¡Œä¹‹å‰æ’å…¥
        sed -i "/# DEER Baseline Dependencies/i\\    \"${dep}\"," "${PYPROJECT}"
        echo "  âœ… æ·»åŠ  ${dep}"
    fi
done

echo "  âœ… ä¾èµ–æ›´æ–°å®Œæˆ"

# ============================================================
# Step 3: åˆ›å»ºæ ‡å‡†è¯„æµ‹å…¥å£è„šæœ¬ run_lcb.sh
# ============================================================
echo ""
echo "[3/6] åˆ›å»ºæ ‡å‡†è¯„æµ‹è„šæœ¬ run_lcb.sh..."

cat > "${BENCHMARK_DIR}/run_lcb.sh" << 'RUNEOF'
#!/bin/bash
# ============================================================
# LiveCodeBench æ ‡å‡†è¯„æµ‹å…¥å£
# ä½¿ç”¨ LiveCodeBench åŸç”Ÿ runner å¯¹ OThink-R1 è®­ç»ƒçš„æ¨¡å‹è¯„æµ‹
#
# ç”¨æ³•:
#   bash benchmark/livecodebench/run_lcb.sh \
#       --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \
#       --model_path /path/to/your/model \
#       --gpu_ids 0
# ============================================================
set -e
eval "$(conda shell.bash hook)"
conda activate othink-r1

# é»˜è®¤å‚æ•°
MODEL=""
MODEL_PATH=""
GPU_IDS="0"
MAX_TOKENS=16289
TEMPERATURE=0.9
CODEGEN_N=1
N=1
RELEASE_VERSION="release_v5"
SCENARIO="codegeneration"
STOP_WORDS="None"
LOCAL_DATA_PATH=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --model) MODEL="$2"; shift 2;;
        --model_path) MODEL_PATH="$2"; shift 2;;
        --gpu_ids) GPU_IDS="$2"; shift 2;;
        --max_tokens) MAX_TOKENS="$2"; shift 2;;
        --temperature) TEMPERATURE="$2"; shift 2;;
        --codegen_n) CODEGEN_N="$2"; shift 2;;
        --n) N="$2"; shift 2;;
        --release_version) RELEASE_VERSION="$2"; shift 2;;
        --local_data) LOCAL_DATA_PATH="$2"; shift 2;;
        *) echo "Unknown option: $1"; exit 1;;
    esac
done

if [ -z "${MODEL}" ]; then
    echo "âŒ è¯·æŒ‡å®šæ¨¡å‹: --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
LCB_DIR="${SCRIPT_DIR}/LiveCodeBench"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"

# å¦‚æœ MODEL_PATH æ˜¯ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºè°ƒç”¨æ—¶çš„ pwd è§£æ
if [ -n "${MODEL_PATH}" ] && [[ "${MODEL_PATH}" != /* ]]; then
    MODEL_PATH="$(cd "$(dirname "${MODEL_PATH}")" && pwd)/$(basename "${MODEL_PATH}")"
fi

export CUDA_VISIBLE_DEVICES="${GPU_IDS}"

echo "=========================================="
echo "  LiveCodeBench æ ‡å‡†è¯„æµ‹"
echo "=========================================="
echo "  æ¨¡å‹: ${MODEL}"
echo "  æ¨¡å‹è·¯å¾„: ${MODEL_PATH:-auto}"
echo "  GPU: ${GPU_IDS}"
echo "  Max Tokens: ${MAX_TOKENS}"
echo "  Temperature: ${TEMPERATURE}"
echo "  Release: ${RELEASE_VERSION}"
echo "=========================================="

# æ„å»ºå‚æ•°
LCB_ARGS=(
    --model "${MODEL}"
    --scenario "${SCENARIO}"
    --max_tokens "${MAX_TOKENS}"
    --release_version "${RELEASE_VERSION}"
    --evaluate
    --codegen_n "${CODEGEN_N}"
    --n "${N}"
    --temperature "${TEMPERATURE}"
    --stop "${STOP_WORDS}"
)

if [ -n "${MODEL_PATH}" ]; then
    LCB_ARGS+=(--local_model_path "${MODEL_PATH}")
fi

if [ -n "${LOCAL_DATA_PATH}" ]; then
    LCB_ARGS+=(--local_dataset_path "${LOCAL_DATA_PATH}")
elif [ -d "${PROJECT_ROOT}/datasets/livecodebench/code_generation_lite" ]; then
    LCB_ARGS+=(--local_dataset_path "${PROJECT_ROOT}/datasets/livecodebench/code_generation_lite")
fi

cd "${LCB_DIR}"
uv run python -m lcb_runner.runner.main "${LCB_ARGS[@]}"

MODEL_NAME=$(basename "${MODEL}")
OUTPUT_FILE="${LCB_DIR}/output/${MODEL_NAME}/Scenario.${SCENARIO}_${CODEGEN_N}_${TEMPERATURE}.json"

if [ -f "${OUTPUT_FILE}" ]; then
    echo ""
    echo "  âœ… è¯„æµ‹å®Œæˆï¼ç»“æœ: ${OUTPUT_FILE}"

    # ç»Ÿè®¡ token é•¿åº¦
    echo ""
    echo "  ç»Ÿè®¡ token é•¿åº¦..."
    uv run python -m lcb_runner.utils.get_length_lcb \
        --model_name "${MODEL}" \
        --file_path "${OUTPUT_FILE}" \
        2>/dev/null || echo "  âš ï¸  token ç»Ÿè®¡å¤±è´¥ï¼ˆä¸å½±å“è¯„æµ‹ç»“æœï¼‰"
else
    echo "  âš ï¸  æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶: ${OUTPUT_FILE}"
fi
RUNEOF
chmod +x "${BENCHMARK_DIR}/run_lcb.sh"
echo "  âœ… run_lcb.sh åˆ›å»ºå®Œæˆ"

# ============================================================
# Step 4: åˆ›å»º DEER é€‚é…è„šæœ¬ deer_lcb.py
# ============================================================
echo ""
echo "[4/6] åˆ›å»º DEER é€‚é…è„šæœ¬ deer_lcb.py..."

cat > "${BENCHMARK_DIR}/deer_lcb.py" << 'DEEREOF'
"""
DEER (Dynamic Early Exit for Reasoning) é€‚é… LiveCodeBench ä»£ç ç”Ÿæˆä»»åŠ¡

æ ¸å¿ƒé€‚é…ç‚¹:
  - åŸå§‹ DEER åœ¨ math reasoning ä¸­ä½¿ç”¨ \\boxed{} ä½œä¸º answer inducing promptï¼Œ
    åœ¨

</think>

å¤„æ£€æµ‹ answer confidence æ¥å†³å®š early exitã€‚
  - ä»£ç ç”Ÿæˆä»»åŠ¡æ²¡æœ‰ \\boxed{}ï¼Œæˆ‘ä»¬æ”¹ä¸º:
    1. answer_prompt: ä½¿ç”¨ "```python\n" æ¥è¯±å¯¼æ¨¡å‹è¾“å‡ºä»£ç 
    2. stop tokens: åœ¨ "```" (ä»£ç å—ç»“æŸ) å¤„æ£€æµ‹ confidence
    3. confidence è®¡ç®—: å¤ç”¨ DEER çš„ avg logprob ç­–ç•¥
  - ç”Ÿæˆå®Œæˆåï¼Œæå–ä»£ç å—ï¼Œè°ƒç”¨ LiveCodeBench çš„è¯„æµ‹ç®¡çº¿è¯„åˆ†

ç”¨æ³•:
  cd OThink-R1 é¡¹ç›®æ ¹ç›®å½•
  python benchmark/livecodebench/deer_lcb.py \
      --model_name_or_path ./models/DeepSeek-R1-Distill-Qwen-7B \
      --threshold 0.95 \
      --max_len 16384 \
      --dataset_path ./datasets/livecodebench/code_generation_lite \
      --release_version release_v5 \
      --gpu_ids 0
"""
import warnings
warnings.filterwarnings("ignore")

import os
import sys
import json
import time
import argparse
import re
import math
import numpy as np
import random

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer
from tqdm import tqdm
from vllm import LLM, SamplingParams

# ---------------------------------------------------------------------------
# å°† LiveCodeBench åŠ å…¥ Python path
# ---------------------------------------------------------------------------
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
LCB_DIR = os.path.join(SCRIPT_DIR, "LiveCodeBench")
sys.path.insert(0, LCB_DIR)

from lcb_runner.benchmarks.code_generation import CodeGenerationProblem
from lcb_runner.evaluation.compute_code_generation_metrics import codegen_metrics

# ---------------------------------------------------------------------------
# DEER æ ¸å¿ƒ: answer confidence è®¡ç®— (å¤ç”¨è‡ª vllm-deer.py)
# ---------------------------------------------------------------------------
def calculate_avg_prob_from_logprobs(logprobs_list, policy='avg1') -> float:
    """ä» vLLM logprobs è®¡ç®—å¹³å‡ token æ¦‚ç‡"""
    num_tokens = len(logprobs_list)
    if num_tokens < 2:
        return 0.0

    total_prob = 0.0
    log_prob_sum = 0.0
    count = 0
    min_prob = 1.0

    for i in range(1, num_tokens):
        if i < len(logprobs_list) and logprobs_list[i]:
            try:
                logprob_obj = list(logprobs_list[i].values())[0]
                if hasattr(logprob_obj, 'logprob'):
                    prob = math.exp(logprob_obj.logprob)
                    min_prob = min(min_prob, prob)
                    total_prob += prob
                    log_prob_sum += math.log(max(prob, 1e-10))
                    count += 1
            except (IndexError, KeyError, AttributeError):
                pass

    if count == 0:
        return 0.0

    if policy == 'min':
        return min_prob
    elif policy == 'avg1':
        return total_prob / count
    elif policy == 'avg2':
        return math.exp(log_prob_sum / count)
    return 0.0

# ---------------------------------------------------------------------------
# ä»£ç æå–: ä»æ¨¡å‹è¾“å‡ºä¸­æå– python ä»£ç 
# ---------------------------------------------------------------------------
def extract_code_from_response(response: str) -> str:
    """ä»æ¨¡å‹å›å¤ä¸­æå– Python ä»£ç å—"""
    # å°è¯•åŒ¹é… ```python ... ``` ä»£ç å—
    pattern = r'```python\s*\n(.*?)```'
    matches = re.findall(pattern, response, re.DOTALL)
    if matches:
        return matches[-1].strip()

    # å°è¯•åŒ¹é… ``` ... ``` ä»£ç å—
    pattern = r'```\s*\n(.*?)```'
    matches = re.findall(pattern, response, re.DOTALL)
    if matches:
        return matches[-1].strip()

    # å¦‚æœ

</think>

ä¹‹åæœ‰å†…å®¹ï¼Œå–

</think>

ä¹‹åçš„éƒ¨åˆ†
    if '

</think>

' in response:
        after_think = response.split('

</think>

')[-1].strip()
        if after_think:
            return after_think

    return response.strip()

# ---------------------------------------------------------------------------
# åŠ è½½ LiveCodeBench æ•°æ®é›†
# ---------------------------------------------------------------------------
def load_lcb_dataset(dataset_path, release_version="release_v5"):
    """åŠ è½½ LiveCodeBench ä»£ç ç”Ÿæˆæ•°æ®é›†"""
    from datasets import load_dataset

    if dataset_path and os.path.isdir(dataset_path):
        dataset = load_dataset(dataset_path, split="test")
    else:
        dataset = load_dataset("livecodebench/code_generation_lite", split="test")

    problems = [CodeGenerationProblem(**item) for item in dataset]

    # æŒ‰ release_version è¿‡æ»¤
    if release_version and release_version != "release_latest":
        # LiveCodeBench çš„ç‰ˆæœ¬è¿‡æ»¤é€»è¾‘
        from lcb_runner.utils.scenarios import Scenario
        pass  # ä¿ç•™å…¨éƒ¨ï¼Œè®© LiveCodeBench å†…éƒ¨å¤„ç†

    return problems

# ---------------------------------------------------------------------------
# ä¸»å‡½æ•°
# ---------------------------------------------------------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="DEER on LiveCodeBench")
    parser.add_argument('--model_name_or_path', type=str, required=True)
    parser.add_argument('--dataset_path', type=str, default=None,
                        help="æœ¬åœ° LiveCodeBench æ•°æ®é›†è·¯å¾„")
    parser.add_argument('--release_version', type=str, default='release_v5')
    parser.add_argument('--threshold', type=float, default=0.95,
                        help="DEER early exit confidence é˜ˆå€¼")
    parser.add_argument('--max_len', type=int, default=16384,
                        help="æœ€å¤§ç”Ÿæˆ token æ•°")
    parser.add_argument('--think_ratio', type=float, default=0.87,
                        help="æ€è€ƒé˜¶æ®µå æ€» token é¢„ç®—çš„æ¯”ä¾‹")
    parser.add_argument('--temperature', type=float, default=0.0)
    parser.add_argument('--top_p', type=float, default=1.0)
    parser.add_argument('--policy', type=str, default='avg1',
                        choices=['min', 'avg1', 'avg2'])
    parser.add_argument('--gpu_ids', type=str, default='0')
    parser.add_argument('--output_dir', type=str, default=None)
    parser.add_argument('--max_judge_steps', type=int, default=10)
    parser.add_argument('--prob_check_max_tokens', type=int, default=30)
    parser.add_argument('--batch_size', type=int, default=2000)
    parser.add_argument('--max_samples', type=int, default=None,
                        help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰")
    parser.add_argument('--no_evaluate', action='store_true',
                        help="åªç”Ÿæˆä¸è¯„æµ‹")
    args = parser.parse_args()
    return args

def main():
    args = parse_args()
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_ids
    available_gpus = args.gpu_ids.split(',')

    print("=" * 60)
    print("  DEER Ã— LiveCodeBench (Code Generation)")
    print("=" * 60)
    print(f"  æ¨¡å‹: {args.model_name_or_path}")
    print(f"  é˜ˆå€¼: {args.threshold}")
    print(f"  æœ€å¤§é•¿åº¦: {args.max_len}")
    print(f"  æ€è€ƒæ¯”ä¾‹: {args.think_ratio}")
    print(f"  ç­–ç•¥: {args.policy}")
    print("=" * 60)

    # ---- åˆå§‹åŒ– vLLM ----
    model_context_len = args.max_len + 8000
    llm_engine = LLM(
        model=args.model_name_or_path,
        tensor_parallel_size=len(available_gpus),
        dtype="bfloat16",
        max_model_len=model_context_len,
        gpu_memory_utilization=0.9,
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path, trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ---- åŠ è½½æ•°æ®é›† ----
    print("\nåŠ è½½ LiveCodeBench æ•°æ®é›†...")
    problems = load_lcb_dataset(args.dataset_path, args.release_version)
    if args.max_samples:
        problems = problems[:args.max_samples]
    print(f"  å…± {len(problems)} é“é¢˜")

    # ---- DEER é€‚é…: ä»£ç ç”Ÿæˆçš„ prompt å’Œ stop tokens ----
    # ä»£ç ç”Ÿæˆä»»åŠ¡çš„ answer inducing prompt
    code_answer_prompt = "\n\n```python\n"

    # æ€è€ƒé˜¶æ®µçš„ stop tokens
    think_stop_tokens = ["Wait", "

</think>

", tokenizer.eos_token]
    # confidence æ£€æŸ¥é˜¶æ®µçš„ stop tokens (ä»£ç å—ç»“æŸ)
    prob_check_stop_tokens = ["```\n", "```"]
    # æœ€ç»ˆå›ç­”é˜¶æ®µçš„ stop tokens
    answer_stop_tokens = [tokenizer.eos_token]

    think_limit = int(args.max_len * args.think_ratio)

    # ---- æ„å»º prompts ----
    sys_prompt = (
        "You are an expert Python programmer. "
        "Solve the given competitive programming problem. "
        "Think step by step inside

<think>

...

</think>

tags, "
        "then provide your final solution as a Python code block."
    )

    formatted_prompts = []
    for prob in problems:
        question_content = prob.question_content
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": question_content},
        ]
        prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        formatted_prompts.append(prompt)

    # ---- DEER ä¸»å¾ªç¯ (ç®€åŒ–ç‰ˆ: æ‰¹é‡å¤„ç†) ----
    # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ DEER é€»è¾‘:
    # Phase 1: ç”Ÿæˆæ€è€ƒè¿‡ç¨‹ï¼Œåœ¨ "Wait" æˆ– "

</think>

" å¤„åœä¸‹
    # Phase 2: ç”¨ code_answer_prompt è¯±å¯¼ä»£ç è¾“å‡ºï¼Œæ£€æŸ¥ confidence
    # Phase 3: å¦‚æœ confidence > threshold æˆ–è¾¾åˆ°é™åˆ¶ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    print("\nå¼€å§‹ DEER æ¨ç†...")
    start_time = time.time()

    all_results = []

    for idx, (prob, prompt) in enumerate(tqdm(
        zip(problems, formatted_prompts), total=len(problems), desc="DEER"
    )):
        thinking_history = ""
        current_seq = prompt
        thinking_steps = 0
        early_exit = False
        too_long = False

        # ---- Phase 1: è¿­ä»£æ€è€ƒ ----
        while True:
            think_tokens_used = len(tokenizer.encode(
                thinking_history, add_special_tokens=False
            ))
            remaining = think_limit - think_tokens_used
            if remaining <= 50:
                too_long = True
                break

            # ç”Ÿæˆä¸€æ®µæ€è€ƒ
            if thinking_steps < args.max_judge_steps:
                stop = think_stop_tokens
            else:
                stop = ["

</think>

", tokenizer.eos_token]

            outputs = llm_engine.generate(
                [current_seq],
                SamplingParams(
                    max_tokens=min(remaining, 4096),
                    temperature=args.temperature,
                    top_p=args.top_p,
                    stop=stop,
                ),
                use_tqdm=False,
            )

            gen_text = outputs[0].outputs[0].text
            gen_ids = outputs[0].outputs[0].token_ids
            last_id = gen_ids[-1] if gen_ids else -1

            thinking_history += gen_text
            current_seq = prompt + thinking_history

            # æ£€æŸ¥æ˜¯å¦è‡ªç„¶ç»“æŸæ€è€ƒ
            think_end_ids = tokenizer.encode("

</think>

", add_special_tokens=False)
            if last_id in think_end_ids:
                break

            # ---- Phase 2: Confidence æ£€æŸ¥ ----
            thinking_steps += 1
            prob_check_prompt = current_seq + code_answer_prompt

            prob_outputs = llm_engine.generate(
                [prob_check_prompt],
                SamplingParams(
                    max_tokens=args.prob_check_max_tokens,
                    stop=prob_check_stop_tokens,
                    logprobs=1,
                ),
                use_tqdm=False,
            )

            if prob_outputs[0].outputs[0].logprobs:
                pred_prob = calculate_avg_prob_from_logprobs(
                    prob_outputs[0].outputs[0].logprobs, args.policy
                )
            else:
                pred_prob = 0.0

            if pred_prob > args.threshold:
                early_exit = True
                break

            # ç»§ç»­æ€è€ƒ
            if not current_seq.rstrip().endswith("Wait"):
                current_seq += "Wait"
                thinking_history += "Wait"

        # ---- Phase 3: ç”Ÿæˆæœ€ç»ˆä»£ç  ----
        final_prompt = prompt + thinking_history
        if not thinking_history.rstrip().endswith("

</think>

"):
            final_prompt += "\n

</think>

\n\n"
        else:
            final_prompt += "\n\n"

        answer_budget = args.max_len - len(
            tokenizer.encode(thinking_history, add_special_tokens=False)
        )
        answer_budget = max(answer_budget, 512)

        final_outputs = llm_engine.generate(
            [final_prompt],
            SamplingParams(
                max_tokens=min(answer_budget, 4096),
                temperature=args.temperature,
                top_p=args.top_p,
                stop=answer_stop_tokens,
            ),
            use_tqdm=False,
        )

        final_text = final_outputs[0].outputs[0].text
        full_response = thinking_history + "\n

</think>

\n\n" + final_text

        # æå–ä»£ç 
        code = extract_code_from_response(full_response)

        all_results.append({
            "question_id": prob.question_id,
            "question_content": prob.question_content,
            "output_list": [full_response],
            "code_list": [code],
            "thinking_steps": thinking_steps,
            "early_exit": early_exit,
            "too_long": too_long,
            "total_tokens": len(tokenizer.encode(full_response, add_special_tokens=False)),
        })

    elapsed = time.time() - start_time
    print(f"\næ¨ç†å®Œæˆ! å…± {len(all_results)} é“é¢˜, è€—æ—¶ {elapsed:.1f}s")

    # ---- ä¿å­˜ç»“æœ ----
    model_name = os.path.basename(args.model_name_or_path)
    if args.output_dir:
        out_dir = args.output_dir
    else:
        out_dir = os.path.join(SCRIPT_DIR, "outputs", "deer", model_name)
    os.makedirs(out_dir, exist_ok=True)

    out_file = os.path.join(
        out_dir,
        f"deer_p{args.threshold}_ratio{args.think_ratio}_len{args.max_len}.json"
    )
    with open(out_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"ç»“æœä¿å­˜åˆ°: {out_file}")

    # ---- ç»Ÿè®¡ ----
    early_exits = sum(1 for r in all_results if r['early_exit'])
    too_longs = sum(1 for r in all_results if r['too_long'])
    avg_tokens = np.mean([r['total_tokens'] for r in all_results])
    avg_steps = np.mean([r['thinking_steps'] for r in all_results])

    print(f"\n============= DEER ç»Ÿè®¡ =============")
    print(f"  æ€»é¢˜æ•°: {len(all_results)}")
    print(f"  Early Exit: {early_exits} ({100*early_exits/len(all_results):.1f}%)")
    print(f"  Too Long:   {too_longs} ({100*too_longs/len(all_results):.1f}%)")
    print(f"  å¹³å‡ tokens: {avg_tokens:.0f}")
    print(f"  å¹³å‡æ€è€ƒæ­¥æ•°: {avg_steps:.1f}")

    # ---- è¯„æµ‹ (å¯é€‰) ----
    if not args.no_evaluate:
        print(f"\nå¼€å§‹ LiveCodeBench è¯„æµ‹...")
        try:
            combined = [([r['output_list'][0]], [r['code_list'][0]]) for r in all_results]
            metrics = codegen_metrics(
                problems[:len(all_results)],
                combined,
                num_process_evaluate=12,
                timeout=6,
            )
            print(f"\n============= LiveCodeBench è¯„æµ‹ç»“æœ =============")
            if isinstance(metrics, tuple) and len(metrics) >= 1:
                print(json.dumps(metrics[0], indent=2))
            else:
                print(json.dumps(metrics, indent=2))

            eval_file = out_file.replace('.json', '_eval.json')
            with open(eval_file, 'w') as f:
                json.dump(metrics, f, indent=2)
            print(f"è¯„æµ‹ç»“æœä¿å­˜åˆ°: {eval_file}")
        except Exception as e:
            print(f"  âš ï¸  è¯„æµ‹å¤±è´¥: {e}")
            print(f"  å¯ä»¥ç¨åæ‰‹åŠ¨è¯„æµ‹")

if __name__ == "__main__":
    main()
DEEREOF
echo "  âœ… deer_lcb.py åˆ›å»ºå®Œæˆ"

# ============================================================
# Step 5: åˆ›å»º CP-Router stub (ä¸é€‚ç”¨äºä»£ç ç”Ÿæˆ)
# ============================================================
echo ""
echo "[5/6] åˆ›å»º CP-Router stub..."

cat > "${BENCHMARK_DIR}/cp_router_lcb_stub.py" << 'CPEOF'
"""
CP-Router Ã— LiveCodeBench Stub

CP-Router æ˜¯ä¸€ä¸ªåŸºäº Conformal Prediction çš„ MCQA è·¯ç”±å™¨ã€‚
å®ƒé€šè¿‡åˆ†æ LLM å¯¹é€‰é¡¹ (A/B/C/D) çš„ softmax æ¦‚ç‡æ¥æ„å»ºé¢„æµ‹é›†ï¼Œ
ç„¶åæ ¹æ®é¢„æµ‹é›†å¤§å°å†³å®šè·¯ç”±åˆ° LLM è¿˜æ˜¯ LRMã€‚

âš ï¸  CP-Router ä¸é€‚ç”¨äºä»£ç ç”Ÿæˆä»»åŠ¡:
  1. ä»£ç ç”Ÿæˆæ˜¯å¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ï¼Œæ²¡æœ‰å›ºå®šé€‰é¡¹é›†åˆ
  2. æ— æ³•è®¡ç®—é€‰é¡¹çº§åˆ«çš„ nonconformity scores
  3. æ— æ³•æ„å»ºæœ‰æ„ä¹‰çš„é¢„æµ‹é›†

å¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆ (æœªæ¥å·¥ä½œ):
  - åŸºäº prompt éš¾åº¦åˆ†ç±»çš„è·¯ç”± (å¦‚ problem rating)
  - åŸºäºé¦– N ä¸ª token çš„ perplexity åšè·¯ç”±å†³ç­–
  - åŸºäºä»£ç ç”Ÿæˆ confidence (å¦‚ pass@k ä¼°è®¡) åšè·¯ç”±

æœ¬æ–‡ä»¶ä»…ä½œä¸ºå ä½ç¬¦ï¼Œè®°å½•ä¸é€‚ç”¨åŸå› ã€‚
"""
import sys

def main():
    print("=" * 60)
    print("  CP-Router Ã— LiveCodeBench")
    print("=" * 60)
    print()
    print("  âŒ CP-Router ä¸é€‚ç”¨äºä»£ç ç”Ÿæˆä»»åŠ¡")
    print()
    print("  åŸå› :")
    print("    - CP-Router åŸºäº MCQA é€‰é¡¹ logits åš conformal prediction")
    print("    - ä»£ç ç”Ÿæˆæ˜¯å¼€æ”¾å¼ä»»åŠ¡ï¼Œæ²¡æœ‰å›ºå®šé€‰é¡¹é›†åˆ")
    print("    - æ— æ³•è®¡ç®—é€‰é¡¹çº§åˆ«çš„ nonconformity scores")
    print()
    print("  æ›¿ä»£æ–¹æ¡ˆ (æœªæ¥å·¥ä½œ):")
    print("    - åŸºäº problem difficulty rating çš„è·¯ç”±")
    print("    - åŸºäº prompt perplexity çš„è·¯ç”±")
    print("    - åŸºäº code generation confidence çš„è·¯ç”±")
    print()
    sys.exit(0)

if __name__ == "__main__":
    main()
CPEOF
echo "  âœ… cp_router_lcb_stub.py åˆ›å»ºå®Œæˆ"

# ============================================================
# Step 6: åˆ›å»º DEER è¿è¡Œè„šæœ¬ + README
# ============================================================
echo ""
echo "[6/6] åˆ›å»ºè¿è¡Œè„šæœ¬å’Œ README..."

# DEER è¿è¡Œè„šæœ¬
cat > "${BENCHMARK_DIR}/run_deer_lcb.sh" << 'DEERSHEOF'
#!/bin/bash
# ============================================================
# DEER Ã— LiveCodeBench è¿è¡Œè„šæœ¬
#
# ç”¨æ³•:
#   bash benchmark/livecodebench/run_deer_lcb.sh \
#       --model /path/to/model \
#       --gpu_ids 0 \
#       --threshold 0.95
# ============================================================
set -e
eval "$(conda shell.bash hook)"
conda activate othink-r1

MODEL_PATH=""
GPU_IDS="0"
THRESHOLD=0.95
MAX_LEN=16384
THINK_RATIO=0.87
POLICY="avg1"
TEMPERATURE=0.0
MAX_SAMPLES=""
LOCAL_DATA=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --model) MODEL_PATH="$2"; shift 2;;
        --gpu_ids) GPU_IDS="$2"; shift 2;;
        --threshold) THRESHOLD="$2"; shift 2;;
        --max_len) MAX_LEN="$2"; shift 2;;
        --think_ratio) THINK_RATIO="$2"; shift 2;;
        --policy) POLICY="$2"; shift 2;;
        --temperature) TEMPERATURE="$2"; shift 2;;
        --max_samples) MAX_SAMPLES="$2"; shift 2;;
        --local_data) LOCAL_DATA="$2"; shift 2;;
        *) echo "Unknown option: $1"; exit 1;;
    esac
done

if [ -z "${MODEL_PATH}" ]; then
    echo "âŒ è¯·æŒ‡å®šæ¨¡å‹è·¯å¾„: --model /path/to/model"
    exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
PROJECT_ROOT="$(cd "${SCRIPT_DIR}/../.." && pwd)"

# è§£æç›¸å¯¹è·¯å¾„
if [[ "${MODEL_PATH}" != /* ]]; then
    MODEL_PATH="$(cd "$(dirname "${MODEL_PATH}")" && pwd)/$(basename "${MODEL_PATH}")"
fi

DEER_ARGS=(
    --model_name_or_path "${MODEL_PATH}"
    --threshold "${THRESHOLD}"
    --max_len "${MAX_LEN}"
    --think_ratio "${THINK_RATIO}"
    --policy "${POLICY}"
    --temperature "${TEMPERATURE}"
    --gpu_ids "${GPU_IDS}"
)

# æ•°æ®é›†è·¯å¾„
if [ -n "${LOCAL_DATA}" ]; then
    DEER_ARGS+=(--dataset_path "${LOCAL_DATA}")
elif [ -d "${PROJECT_ROOT}/datasets/livecodebench/code_generation_lite" ]; then
    DEER_ARGS+=(--dataset_path "${PROJECT_ROOT}/datasets/livecodebench/code_generation_lite")
fi

if [ -n "${MAX_SAMPLES}" ]; then
    DEER_ARGS+=(--max_samples "${MAX_SAMPLES}")
fi

cd "${PROJECT_ROOT}"
uv run python "${SCRIPT_DIR}/deer_lcb.py" "${DEER_ARGS[@]}"
DEERSHEOF
chmod +x "${BENCHMARK_DIR}/run_deer_lcb.sh"

# ä¸‹è½½æ•°æ®é›†è„šæœ¬
cat > "${BENCHMARK_DIR}/download_data.sh" << 'DLEOF'
#!/bin/bash
# ä¸‹è½½ LiveCodeBench æ•°æ®é›†
set -e
eval "$(conda shell.bash hook)"
conda activate othink-r1

PROJECT_ROOT="$(cd "$(dirname "$0")/../.." && pwd)"
DATA_DIR="${PROJECT_ROOT}/datasets/livecodebench/code_generation_lite"

export HF_ENDPOINT=https://hf-mirror.com

if [ -d "${DATA_DIR}" ]; then
    echo "  âš ï¸  æ•°æ®é›†å·²å­˜åœ¨: ${DATA_DIR}"
else
    echo "  ğŸ“¦ ä¸‹è½½ LiveCodeBench æ•°æ®é›†..."
    mkdir -p "${DATA_DIR}"
    uv run huggingface-cli download \
        --repo-type dataset \
        livecodebench/code_generation_lite \
        --local-dir "${DATA_DIR}" \
        --local-dir-use-symlinks False \
        --resume-download
    echo "  âœ… ä¸‹è½½å®Œæˆ: ${DATA_DIR}"
fi
DLEOF
chmod +x "${BENCHMARK_DIR}/download_data.sh"

# README
cat > "${BENCHMARK_DIR}/README.md" << 'READMEEOF'
# LiveCodeBench Benchmark for OThink-R1

å°† [LiveCodeBench](https://github.com/cynthia-shengjia/LiveCodeBench) é›†æˆåˆ° OThink-R1 é¡¹ç›®ä¸­ï¼Œ
æ”¯æŒæ ‡å‡†è¯„æµ‹å’Œ DEER early-exit é€‚é…ã€‚

## ç›®å½•ç»“æ„
benchmark/livecodebench/
â”œâ”€â”€ LiveCodeBench/ # LiveCodeBench æºç  (git clone)
â”œâ”€â”€ run_lcb.sh # æ ‡å‡†è¯„æµ‹å…¥å£
â”œâ”€â”€ run_deer_lcb.sh # DEER é€‚é…è¿è¡Œè„šæœ¬
â”œâ”€â”€ deer_lcb.py # DEER Ã— LiveCodeBench é€‚é…ä»£ç 
â”œâ”€â”€ cp_router_lcb_stub.py # CP-Router ä¸é€‚ç”¨æ ‡è®°
â”œâ”€â”€ download_data.sh # æ•°æ®é›†ä¸‹è½½è„šæœ¬
â””â”€â”€ README.md # æœ¬æ–‡ä»¶