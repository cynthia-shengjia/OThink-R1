#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OThink-R1 CLI â€” ä¸€ç«™å¼ LLM è¯„æµ‹å…¥å£
=====================================
æ”¾ç½®äºé¡¹ç›®æ ¹ç›®å½•: OThink-R1/othink_cli.py

ç”¨æ³•:
  python othink_cli.py download-data   --datasets all
  python othink_cli.py download-model  --model Qwen/Qwen2.5-7B-Instruct
  python othink_cli.py eval            --model Qwen2.5-0.5B-Instruct --datasets math aime --gpu_ids 0,1,2,3
  python othink_cli.py eval-deer       --model Qwen2.5-0.5B-Instruct --datasets math aime --gpu_ids 0,1
  python othink_cli.py eval-cp-router  --llm_model Qwen2.5-0.5B-Instruct --datasets math aime --gpu_ids 0
  python othink_cli.py eval-lcb        --model Qwen2.5-0.5B-Instruct --mode standard --gpu_ids 0
  python othink_cli.py eval-all        --model Qwen2.5-0.5B-Instruct --gpu_ids 0,1,2,3,4,5,6,7
"""

import argparse
import os
import shutil
import subprocess
import sys
import time
import threading
from pathlib import Path
from collections import deque
from dataclasses import dataclass, field
from typing import List, Optional, Dict

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å…¨å±€å¸¸é‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = Path(__file__).resolve().parent

# æ•°æ®é›†æ³¨å†Œè¡¨: key â†’ { repo, local_dir, deer_name, standard_hydra_name }
DATASET_REGISTRY = {
    "math": {
        "repo": "DigitalLearningGmbH/MATH-lighteval",
        "local": "datasets/MATH",
        "deer_name": "math_hf",           # DEER æ ¼å¼æ•°æ®é›†å (åœ¨ baseline/deer/data/ ä¸‹)
        "standard_name": "MATHBench",      # OThinkR1Training Hydra é…ç½®å
    },
    "aime": {
        "repo": "AI-MO/aimo-validation-aime",
        "local": "datasets/AIME",
        "deer_name": "aime_hf",
        "standard_name": "AIME",
    },
    "asdiv": {
        "repo": "EleutherAI/asdiv",
        "local": "datasets/ASDIV",
        "deer_name": "asdiv_hf",
        "standard_name": "ASDIV",
    },
    "livecodebench": {
        "repo": "livecodebench/code_generation_lite",
        "local": "datasets/livecodebench/code_generation_lite",
        "deer_name": None,
        "standard_name": None,
    },
    "gsm8k": {
        "repo": "openai/gsm8k",
        "local": "datasets/GSM8K",
        "deer_name": "gsm8k",
        "standard_name": None,
    },
    "gpqa": {
        "repo": "Idavidrein/gpqa",
        "local": "datasets/GPQA",
        "deer_name": "gpqa",
        "standard_name": None,
    },
    
    "openbookqa": {
    "repo": "allenai/openbookqa",
    "local": "datasets/OpenBookQA",
    "deer_name": None,
    "standard_name": "OpenBookQA",
    },
    
    "commonsenseqa": {
        "repo": "tau/commonsense_qa",
        "local": "datasets/CommonsenseQA",
        "deer_name": None,
        "standard_name": "CommonsenseQA",
    }
}

DEER_CONVERTIBLE = {"math", "aime", "asdiv"}  # convert_hf_to_deer.py æ”¯æŒçš„

DEFAULT_HF_MIRROR = "https://hf-mirror.com"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ—¥å¿—è¾…åŠ©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Log:
    @staticmethod
    def info(msg):    print(f"â„¹ï¸  {msg}", flush=True)
    @staticmethod
    def ok(msg):      print(f"âœ… {msg}", flush=True)
    @staticmethod
    def warn(msg):    print(f"âš ï¸  {msg}", flush=True)
    @staticmethod
    def err(msg):     print(f"âŒ {msg}", file=sys.stderr, flush=True)
    @staticmethod
    def run(msg):     print(f"ğŸš€ {msg}", flush=True)
    @staticmethod
    def dl(msg):      print(f"ğŸ“¥ {msg}", flush=True)
    @staticmethod
    def gpu(msg):     print(f"ğŸ–¥ï¸  {msg}", flush=True)
    @staticmethod
    def time(msg):    print(f"â±ï¸  {msg}", flush=True)
    @staticmethod
    def done(msg):    print(f"ğŸ‰ {msg}", flush=True)
    @staticmethod
    def task(msg):    print(f"ğŸ“‹ {msg}", flush=True)
    @staticmethod
    def skip(msg):    print(f"â­ï¸  {msg}", flush=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPU å¹¶è¡Œè°ƒåº¦å™¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class TaskItem:
    name: str
    cmd: List[str]
    env_extra: Dict[str, str] = field(default_factory=dict)
    cwd: Optional[str] = None
    gpu_count: int = 1


@dataclass
class TaskResult:
    name: str
    returncode: int
    elapsed: float
    gpu_ids: List[int]


class GPUScheduler:
    """ç»´æŠ¤ç©ºé—² GPU æ± ï¼Œè‡ªåŠ¨å°†ä»»åŠ¡åˆ†é…åˆ°ç©ºé—²å¡ä¸Šå¹¶è¡Œè¿è¡Œã€‚"""

    def __init__(self, gpu_ids: List[int]):
        self._all = list(gpu_ids)
        self._free: deque = deque(gpu_ids)
        self._lock = threading.Lock()
        self._results: List[TaskResult] = []
        self._rlock = threading.Lock()

    def run_all(self, tasks: List[TaskItem]) -> List[TaskResult]:
        queue: deque = deque(tasks)
        active: List[threading.Thread] = []
        Log.task(f"å…± {len(tasks)} ä¸ªä»»åŠ¡, å¯ç”¨ GPU: {self._all}")

        while queue or active:
            active = [t for t in active if t.is_alive()]
            scheduled = True
            while scheduled and queue:
                scheduled = False
                task = queue[0]
                alloc = self._try_alloc(task.gpu_count)
                if alloc is not None:
                    queue.popleft()
                    t = threading.Thread(target=self._exec, args=(task, alloc), daemon=True)
                    t.start()
                    active.append(t)
                    scheduled = True
            time.sleep(0.5)
        return self._results

    def _try_alloc(self, n):
        with self._lock:
            if len(self._free) >= n:
                return [self._free.popleft() for _ in range(n)]
        return None

    def _release(self, ids):
        with self._lock:
            self._free.extend(ids)

    def _exec(self, task: TaskItem, gpu_ids: List[int]):
        cuda = ",".join(str(g) for g in gpu_ids)
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = cuda
        env.update(task.env_extra)

        Log.gpu(f"[START] {task.name}  â†’  GPU [{cuda}]")
        t0 = time.time()
        try:
            proc = subprocess.Popen(
                task.cmd, env=env,
                cwd=task.cwd or str(PROJECT_ROOT),
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True,
            )
            prefix = f"[{task.name}|GPU {cuda}]"
            for line in proc.stdout:
                print(f"  {prefix} {line}", end="", flush=True)
            proc.wait()
            rc = proc.returncode
        except Exception as e:
            Log.err(f"ä»»åŠ¡ {task.name} å¯åŠ¨å¤±è´¥: {e}")
            rc = -1

        elapsed = time.time() - t0
        self._release(gpu_ids)
        r = TaskResult(name=task.name, returncode=rc, elapsed=elapsed, gpu_ids=gpu_ids)
        with self._rlock:
            self._results.append(r)
        icon = "âœ…" if rc == 0 else "âŒ"
        Log.time(f"[DONE] {icon} {task.name}  è€—æ—¶ {elapsed:.1f}s  rc={rc}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¾…åŠ©å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_gpu_ids(s: str) -> List[int]:
    return [int(x.strip()) for x in s.split(",") if x.strip()]


def resolve_model(name: str) -> Path:
    """models/ ä¸‹çš„åç§° æˆ– ç»å¯¹è·¯å¾„"""
    p = PROJECT_ROOT / "models" / name
    if p.exists():
        return p
    p2 = Path(name)
    if p2.exists():
        return p2.resolve()
    Log.err(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {p} æˆ– {name}")
    sys.exit(1)


def run_cmd(cmd, env_extra=None, cwd=None):
    env = os.environ.copy()
    if env_extra:
        env.update(env_extra)
    Log.run(f"$ {' '.join(cmd)}")
    proc = subprocess.Popen(cmd, env=env, cwd=cwd or str(PROJECT_ROOT),
                            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    for line in proc.stdout:
        print(f"  | {line}", end="", flush=True)
    proc.wait()
    return proc.returncode


def print_summary(results: List[TaskResult]):
    print("\n" + "=" * 72)
    Log.done("è¯„æµ‹ç»“æœæ±‡æ€»")
    print("-" * 72)
    print(f"  {'ä»»åŠ¡å':<40} {'GPU':<10} {'è€—æ—¶':>8} {'çŠ¶æ€':>6}")
    print("-" * 72)
    for r in sorted(results, key=lambda x: x.name):
        st = "âœ…" if r.returncode == 0 else f"âŒ rc={r.returncode}"
        gs = ",".join(str(g) for g in r.gpu_ids)
        print(f"  {r.name:<40} {gs:<10} {r.elapsed:>7.1f}s {st}")
    print("=" * 72)
    failed = [r for r in results if r.returncode != 0]
    if failed:
        Log.err(f"{len(failed)} ä¸ªä»»åŠ¡å¤±è´¥!")
    else:
        Log.done("æ‰€æœ‰ä»»åŠ¡å‡å·²æˆåŠŸå®Œæˆ!")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# download-data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_download_data(args):
    if "all" in args.datasets:
        targets = list(DATASET_REGISTRY.keys())
    else:
        targets = [d.lower() for d in args.datasets]

    hf_mirror = args.hf_mirror
    env_hf = {"HF_ENDPOINT": hf_mirror} if hf_mirror else {}
    Log.info(f"HuggingFace é•œåƒ: {hf_mirror or 'å®˜æ–¹æº'}")

    downloaded = []
    for ds in targets:
        meta = DATASET_REGISTRY.get(ds)
        if not meta:
            Log.err(f"æœªçŸ¥æ•°æ®é›†: {ds}. å¯é€‰: {', '.join(DATASET_REGISTRY.keys())}, all")
            continue

        local = PROJECT_ROOT / meta["local"]
        if local.exists() and any(local.iterdir()):
            Log.skip(f"[{ds}] å·²å­˜åœ¨äº {local}ï¼Œè·³è¿‡")
            downloaded.append(ds)
            continue

        Log.dl(f"ä¸‹è½½ [{ds}]: {meta['repo']} â†’ {local}")
        local.mkdir(parents=True, exist_ok=True)

        rc = run_cmd([
            "uv", "run", "huggingface-cli", "download",
            "--repo-type", "dataset",
            meta["repo"],
            "--local-dir", str(local),
            "--local-dir-use-symlinks", "False",
            "--resume-download",
        ], env_extra=env_hf)

        if rc != 0:
            Log.err(f"[{ds}] ä¸‹è½½å¤±è´¥ (rc={rc})")
            continue
        Log.ok(f"[{ds}] ä¸‹è½½å®Œæˆ")
        downloaded.append(ds)

    # DEER æ ¼å¼è½¬æ¢
    convert_py = PROJECT_ROOT / "baseline" / "deer" / "scripts" / "convert_hf_to_deer.py"
    if convert_py.exists():
        deer_targets = [d for d in downloaded if d in DEER_CONVERTIBLE]
        if deer_targets:
            Log.run("ğŸ¦Œ è½¬æ¢ DEER æ ¼å¼...")
            rc = run_cmd([
                "uv", "run", "python", str(convert_py),
                "--hf_dir", str(PROJECT_ROOT / "datasets"),
                "--output_dir", str(PROJECT_ROOT / "baseline" / "deer" / "data"),
                "--dataset", "all",
            ])
            if rc == 0:
                Log.ok("DEER æ ¼å¼è½¬æ¢å®Œæˆ")
            else:
                Log.warn(f"DEER è½¬æ¢å¤±è´¥ (rc={rc}), å¯æ‰‹åŠ¨è¿è¡Œ")

    Log.done("æ•°æ®é›†ä¸‹è½½æµç¨‹å®Œæ¯•!")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# download-model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_download_model(args):
    models_dir = PROJECT_ROOT / "models"
    models_dir.mkdir(parents=True, exist_ok=True)

    save_name = args.name or args.model.rstrip("/").split("/")[-1]
    target = models_dir / save_name

    # å¹‚ç­‰
    if target.exists() and (target.is_symlink() or any(target.iterdir())):
        Log.skip(f"æ¨¡å‹ [{save_name}] å·²å­˜åœ¨äº {target}ï¼Œè·³è¿‡")
        return

    # æœ¬åœ°è·¯å¾„ â†’ è½¯é“¾æ¥
    if args.local_path:
        src = Path(args.local_path).resolve()
        if not src.exists():
            Log.err(f"æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {src}")
            sys.exit(1)
        Log.info(f"åˆ›å»ºè½¯é“¾æ¥: {target} â†’ {src}")
        target.symlink_to(src)
        Log.ok(f"æ¨¡å‹ [{save_name}] å·²é“¾æ¥")
        return

    # HuggingFace ä¸‹è½½
    Log.dl(f"ä¸‹è½½æ¨¡å‹: {args.model} â†’ {target}")
    target.mkdir(parents=True, exist_ok=True)
    env_hf = {"HF_ENDPOINT": args.hf_mirror} if args.hf_mirror else {}

    rc = run_cmd([
        "uv", "run", "huggingface-cli", "download",
        args.model,
        "--local-dir", str(target),
        "--local-dir-use-symlinks", "False",
        "--resume-download",
    ], env_extra=env_hf)

    if rc != 0:
        Log.err(f"æ¨¡å‹ä¸‹è½½å¤±è´¥ (rc={rc})")
        sys.exit(1)
    Log.ok(f"æ¨¡å‹ [{save_name}] ä¸‹è½½å®Œæˆ â†’ {target}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eval â€” æ ‡å‡†è¯„æµ‹ (OThinkR1Training/eval.py + Hydra)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_eval(args):
    model_path = resolve_model(args.model)
    gpu_ids = parse_gpu_ids(args.gpu_ids)
    datasets = [d.lower() for d in args.datasets]

    eval_py = PROJECT_ROOT / "OThinkR1Training" / "eval.py"
    if not eval_py.exists():
        Log.err(f"è¯„æµ‹è„šæœ¬ä¸å­˜åœ¨: {eval_py}")
        sys.exit(1)

    tasks = []
    for ds in datasets:
        meta = DATASET_REGISTRY.get(ds)
        if not meta or not meta.get("standard_name"):
            Log.warn(f"[{ds}] ä¸æ”¯æŒæ ‡å‡†è¯„æµ‹ (æ—  Hydra é…ç½®), è·³è¿‡")
            continue

        hydra_data = meta["standard_name"]
        # æ¨æ–­ Hydra model config å (models/Qwen2.5-0.5B-Instruct â†’ Qwen2.5-0.5B-Instruct)
        model_name = model_path.name

        cmd = [
            "uv", "run", "python", str(eval_py),
            f"model={model_name}",
            f"model.path={model_path}",
            f"model.inference.tensor_parallel_size=1",
            f"model.inference.gpu_memory_utilization=0.9",
            f"+model.inference.repetition_penalty=1.0",
            f"model.inference.temperature={args.temperature}",
            f"model.inference.top_p={args.top_p}",
            f"model.inference.max_tokens={args.max_tokens}",
            f'+model.mode="test"',
            f"data={hydra_data}",
        ]

        tasks.append(TaskItem(name=f"standard-{ds}", cmd=cmd,
                              cwd=str(PROJECT_ROOT / "OThinkR1Training")))

    if not tasks:
        Log.err("æ²¡æœ‰æœ‰æ•ˆçš„è¯„æµ‹ä»»åŠ¡")
        sys.exit(1)

    scheduler = GPUScheduler(gpu_ids)
    results = scheduler.run_all(tasks)
    print_summary(results)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eval-deer â€” DEER æ—©é€€è¯„æµ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_eval_deer(args):
    """DEER æ—©é€€è¯„æµ‹ â€” æ¨ç† + è¯„ä¼° (ç”Ÿæˆ *_othink_eval.json)"""
    model_path = resolve_model(args.model)
    gpu_ids = parse_gpu_ids(args.gpu_ids)
    datasets = [d.lower() for d in args.datasets]

    deer_py = PROJECT_ROOT / "baseline" / "deer" / "vllm-deer.py"
    eval_py = PROJECT_ROOT / "baseline" / "deer" / "scripts" / "eval_with_othink.py"
    check_py = PROJECT_ROOT / "baseline" / "deer" / "check_fixed.py"

    if not deer_py.exists():
        Log.err(f"DEER è„šæœ¬ä¸å­˜åœ¨: {deer_py}")
        sys.exit(1)

    tasks = []
    for ds in datasets:
        meta = DATASET_REGISTRY.get(ds)
        if not meta:
            Log.warn(f"æœªçŸ¥æ•°æ®é›†: {ds}, è·³è¿‡")
            continue

        deer_name = meta.get("deer_name")
        if not deer_name:
            Log.warn(f"[{ds}] ä¸æ”¯æŒ DEER è¯„æµ‹, è·³è¿‡")
            continue

        # æ£€æŸ¥ DEER æ•°æ®æ˜¯å¦å­˜åœ¨
        deer_data_dir = PROJECT_ROOT / "baseline" / "deer" / "data"
        deer_data_file = deer_data_dir / deer_name / "test.jsonl"
        if not deer_data_file.exists():
            Log.warn(f"DEER æ•°æ®ä¸å­˜åœ¨: {deer_data_file}")
            Log.info(f"è¯·å…ˆè¿è¡Œ: python othink_cli.py download-data --datasets {ds}")
            continue

        output_dir = PROJECT_ROOT / "baseline" / "deer" / "outputs"
        model_basename = model_path.name  # e.g. Qwen2.5-0.5B-Instruct
        threshold = args.threshold
        max_len = args.max_len

        # æ„å»ºè¾“å‡ºæ–‡ä»¶å (ä¸ vllm-deer.py ç”Ÿæˆçš„ä¸€è‡´)
        output_pattern = (
            f"greedy_p{threshold}_ratio0.9_len{max_len}_"
            f"temperature0.0_run_time1_no_thinking0_rep0_points1_policyavg1.jsonl"
        )
        expected_output = output_dir / model_basename / deer_name / output_pattern

        # æ„å»º bash -c ä¸²è”å‘½ä»¤: æ¨ç† â†’ æŸ¥æ‰¾è¾“å‡º â†’ è¯„ä¼°
        # è¿™æ ·ä¸¤æ­¥åœ¨åŒä¸€ä¸ªå­è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œå…±äº« CUDA_VISIBLE_DEVICES
        bash_script = f'''
set -e

echo "=========================================="
echo "  DEER æ¨ç†: {deer_name}"
echo "=========================================="

cd "{PROJECT_ROOT}"

# Step 1: æ¨ç†
uv run python "{deer_py}" \\
    --model_name_or_path "{model_path}" \\
    --dataset_dir "{deer_data_dir}" \\
    --dataset "{deer_name}" \\
    --threshold {threshold} \\
    --max-len {max_len} \\
    --think_ratio 0.9 \\
    --temperature 0.0 \\
    --top_p 1.0 \\
    --policy "avg1" \\
    --batch_size 2000 \\
    --output_path "{output_dir}" \\
    --no_thinking 0 \\
    --rep 0 \\
    --points 1 \\
    --af 0 \\
    --max_judge_steps 10 \\
    --prob_check_max_tokens 20 \\
    --run_time 1

echo ""
echo "  âœ… DEER æ¨ç†å®Œæˆ"

# Step 2: æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE=$(find "{output_dir}" -name "*.jsonl" -path "*{deer_name}*" 2>/dev/null | sort -t/ -k+1 | tail -1)

if [ -z "$OUTPUT_FILE" ]; then
    echo "  âš ï¸  æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶"
    exit 1
fi

echo "  è¾“å‡ºæ–‡ä»¶: $OUTPUT_FILE"

# Step 3: DEER è‡ªå¸¦è¯„ä¼° (check_fixed.py)
echo ""
echo "=========================================="
echo "  DEER è‡ªå¸¦è¯„ä¼°"
echo "=========================================="
'''

        # check_fixed.py éœ€è¦åŸå§‹æ•°æ®é›†å (ä¸å¸¦ _hf åç¼€çš„ä¹Ÿè¡Œ)
        if check_py.exists():
            bash_script += f'''
cd "{PROJECT_ROOT}/baseline/deer"
uv run python "{check_py}" \\
    --model_name_or_path "{model_path}" \\
    --data_name "{deer_name}" \\
    --data_dir "{deer_data_dir}" \\
    --generation_path "$OUTPUT_FILE" \\
    2>&1 || echo "  âš ï¸  DEER è‡ªå¸¦è¯„ä¼°å¤±è´¥ (ä¸å½±å“åç»­)"
'''

        # eval_with_othink.py ç”Ÿæˆ *_othink_eval.json
        if eval_py.exists():
            bash_script += f'''
# Step 4: OThink-R1 Verifier è¯„ä¼° (ç”Ÿæˆ *_othink_eval.json)
echo ""
echo "=========================================="
echo "  OThink-R1 Verifier è¯„ä¼°"
echo "=========================================="
cd "{PROJECT_ROOT}"
uv run python "{eval_py}" \\
    --generation_path "$OUTPUT_FILE" \\
    --dataset "{ds}" \\
    2>&1 || echo "  âš ï¸  OThink-R1 è¯„ä¼°å¤±è´¥"

echo ""
echo "=========================================="
echo "  âœ… DEER è¯„æµ‹å®Œæˆ: {deer_name}"
echo "=========================================="
'''

        cmd = ["bash", "-c", bash_script]
        tasks.append(TaskItem(name=f"deer-{ds}", cmd=cmd))

    if not tasks:
        Log.err("æ²¡æœ‰æœ‰æ•ˆçš„ DEER è¯„æµ‹ä»»åŠ¡")
        sys.exit(1)

    scheduler = GPUScheduler(gpu_ids)
    results = scheduler.run_all(tasks)
    print_summary(results)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eval-cp-router â€” CP-Router è¯„æµ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_eval_cp_router(args):
    llm_model = resolve_model(args.llm_model)
    lrm_model = resolve_model(args.lrm_model) if args.lrm_model else llm_model
    gpu_ids = parse_gpu_ids(args.gpu_ids)
    datasets = [d.lower() for d in args.datasets]

    cp_script = PROJECT_ROOT / "baseline" / "cp-router" / "test_cp_router.py"
    if not cp_script.exists():
        Log.err(f"CP-Router è„šæœ¬ä¸å­˜åœ¨: {cp_script}")
        sys.exit(1)

    tasks = []
    for ds in datasets:
        cmd = [
            "uv", "run", "python", str(cp_script),
            "--model_path", str(llm_model),
            "--datasets_dir", str(PROJECT_ROOT / "datasets"),
            "--dataset", ds,
            "--max_samples", str(args.max_samples),
            "--batch_size", str(args.batch_size),
            "--tau", str(args.tau),
            "--beta", str(args.beta),
        ]
        if args.skip_lrm:
            cmd.append("--skip_lrm")
        else:
            cmd.extend(["--lrm_max_tokens", str(args.lrm_max_tokens)])

        tasks.append(TaskItem(name=f"cp-router-{ds}", cmd=cmd,
                              cwd=str(PROJECT_ROOT / "baseline" / "cp-router")))

    if not tasks:
        Log.err("æ²¡æœ‰æœ‰æ•ˆçš„ CP-Router è¯„æµ‹ä»»åŠ¡")
        sys.exit(1)

    scheduler = GPUScheduler(gpu_ids)
    results = scheduler.run_all(tasks)
    print_summary(results)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eval-lcb â€” LiveCodeBench è¯„æµ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_eval_lcb(args):
    model_path = resolve_model(args.model)
    gpu_ids = parse_gpu_ids(args.gpu_ids)

    lcb_dir = PROJECT_ROOT / "benchmark" / "livecodebench"

    if args.mode == "standard":
        script = lcb_dir / "run_standard.sh"
        cmd = [
            "bash", str(script),
            "--model_path", str(model_path),
            "--gpu_ids", ",".join(str(g) for g in gpu_ids),
        ]
        if args.max_problems > 0:
            cmd.extend(["--max_problems", str(args.max_problems)])
    elif args.mode == "deer":
        script = lcb_dir / "run_deer.sh"
        cmd = [
            "bash", str(script),
            "--model_path", str(model_path),
            "--gpu_ids", ",".join(str(g) for g in gpu_ids),
            "--threshold", str(args.threshold),
        ]
        if args.max_problems > 0:
            cmd.extend(["--max_problems", str(args.max_problems)])
    else:
        Log.err(f"æœªçŸ¥ LCB æ¨¡å¼: {args.mode}")
        sys.exit(1)

    if not script.exists():
        Log.err(f"LCB è„šæœ¬ä¸å­˜åœ¨: {script}")
        sys.exit(1)

    # LCB ç›´æ¥è¿è¡Œï¼Œä¸èµ°è°ƒåº¦å™¨ (å› ä¸º GPU å·²åœ¨è„šæœ¬å†…è®¾ç½®)
    Log.run(f"LiveCodeBench [{args.mode}] è¯„æµ‹")
    rc = run_cmd(cmd)
    if rc == 0:
        Log.ok("LiveCodeBench è¯„æµ‹å®Œæˆ")
    else:
        Log.err(f"LiveCodeBench è¯„æµ‹å¤±è´¥ (rc={rc})")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# eval-all â€” ä¸€é”®å…¨é‡è¯„æµ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_eval_all(args):
    model_path = resolve_model(args.model)
    gpu_ids = parse_gpu_ids(args.gpu_ids)
    methods = [m.strip().lower() for m in args.methods.split(",")]
    datasets = [d.strip().lower() for d in args.datasets]

    lrm_model = resolve_model(args.lrm_model) if args.lrm_model else model_path
    threshold = args.threshold
    max_len = args.max_len

    tasks = []

    for method in methods:
        if method == "standard":
            eval_py = PROJECT_ROOT / "OThinkR1Training" / "eval.py"
            if not eval_py.exists():
                Log.warn("æ ‡å‡†è¯„æµ‹è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡")
                continue
            for ds in datasets:
                meta = DATASET_REGISTRY.get(ds)
                if not meta or not meta.get("standard_name"):
                    continue
                model_name = model_path.name
                cmd = [
                    "uv", "run", "python", str(eval_py),
                    f"model={model_name}",
                    f"model.path={model_path}",
                    f"model.inference.tensor_parallel_size=1",
                    f"model.inference.gpu_memory_utilization=0.9",
                    f"+model.inference.repetition_penalty=1.0",
                    f"model.inference.temperature=0.9",
                    f"model.inference.top_p=0.95",
                    f"model.inference.max_tokens=4096",
                    f'+model.mode="test"',
                    f"data={meta['standard_name']}",
                ]
                tasks.append(TaskItem(name=f"standard-{ds}", cmd=cmd,
                                      cwd=str(PROJECT_ROOT / "OThinkR1Training")))

        elif method == "deer":
            deer_py = PROJECT_ROOT / "baseline" / "deer" / "vllm-deer.py"
            if not deer_py.exists():
                Log.warn("DEER è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡")
                continue
            for ds in datasets:
                meta = DATASET_REGISTRY.get(ds)
                if not meta or not meta.get("deer_name"):
                    continue
                deer_name = meta["deer_name"]
                deer_data = PROJECT_ROOT / "baseline" / "deer" / "data" / deer_name / "test.jsonl"
                if not deer_data.exists():
                    Log.warn(f"DEER æ•°æ® [{deer_name}] ä¸å­˜åœ¨, è·³è¿‡")
                    continue
                cmd = [
                    "uv", "run", "python", str(deer_py),
                    "--model_name_or_path", str(model_path),
                    "--dataset_dir", str(PROJECT_ROOT / "baseline" / "deer" / "data"),
                    "--dataset", deer_name,
                    "--threshold", str(threshold),
                    "--max-len", str(max_len),
                    "--think_ratio", "0.9",
                    "--temperature", "0.0",
                    "--top_p", "1.0",
                    "--policy", "avg1",
                    "--batch_size", "2000",
                    "--output_path", str(PROJECT_ROOT / "baseline" / "deer" / "outputs"),
                    "--no_thinking", "0", "--rep", "0", "--points", "1",
                    "--af", "0", "--max_judge_steps", "10",
                    "--prob_check_max_tokens", "20", "--run_time", "1",
                ]
                tasks.append(TaskItem(name=f"deer-{ds}", cmd=cmd))

        elif method == "cp-router":
            cp_script = PROJECT_ROOT / "baseline" / "cp-router" / "test_cp_router.py"
            if not cp_script.exists():
                Log.warn("CP-Router è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡")
                continue
            for ds in datasets:
                cmd = [
                    "uv", "run", "python", str(cp_script),
                    "--model_path", str(model_path),
                    "--datasets_dir", str(PROJECT_ROOT / "datasets"),
                    "--dataset", ds,
                    "--skip_lrm",
                ]
                tasks.append(TaskItem(name=f"cp-router-{ds}", cmd=cmd,
                                      cwd=str(PROJECT_ROOT / "baseline" / "cp-router")))

        elif method == "lcb-standard":
            script = PROJECT_ROOT / "benchmark" / "livecodebench" / "lcb_eval.py"
            if not script.exists():
                Log.warn("LCB æ ‡å‡†è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡")
                continue
            env = {"PYTHONPATH": str(PROJECT_ROOT / "benchmark" / "livecodebench" / "LiveCodeBench")}
            cmd = [
                "uv", "run", "python", str(script),
                "--model_path", str(model_path),
                "--dataset_path", str(PROJECT_ROOT / "datasets" / "livecodebench" / "code_generation_lite"),
            ]
            tasks.append(TaskItem(name="lcb-standard", cmd=cmd, env_extra=env))

        elif method == "lcb-deer":
            script = PROJECT_ROOT / "benchmark" / "livecodebench" / "deer_lcb.py"
            if not script.exists():
                Log.warn("LCB-DEER è„šæœ¬ä¸å­˜åœ¨, è·³è¿‡")
                continue
            env = {"PYTHONPATH": str(PROJECT_ROOT / "benchmark" / "livecodebench" / "LiveCodeBench")}
            cmd = [
                "uv", "run", "python", str(script),
                "--model_path", str(model_path),
                "--dataset_path", str(PROJECT_ROOT / "datasets" / "livecodebench" / "code_generation_lite"),
                "--threshold", str(threshold),
            ]
            tasks.append(TaskItem(name="lcb-deer", cmd=cmd, env_extra=env))

        else:
            Log.warn(f"æœªçŸ¥æ–¹æ³•: {method}, è·³è¿‡")

    if not tasks:
        Log.err("æ²¡æœ‰ç”Ÿæˆä»»ä½•è¯„æµ‹ä»»åŠ¡")
        sys.exit(1)

    Log.run(f"ğŸ“Š å…± {len(tasks)} ä¸ªä»»åŠ¡, {len(gpu_ids)} å¼  GPU")
    for i, t in enumerate(tasks, 1):
        Log.task(f"  {i:>3}. {t.name}")
    print()

    t0 = time.time()
    scheduler = GPUScheduler(gpu_ids)
    results = scheduler.run_all(tasks)
    total = time.time() - t0

    print_summary(results)
    Log.time(f"æ€»è€—æ—¶: {total:.1f}s ({total/60:.1f} min)")




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä»¥ä¸‹ä»£ç è¿½åŠ åˆ° othink_cli.py ä¸­
# åœ¨ cmd_eval_all å‡½æ•°ä¹‹åã€build_parser å‡½æ•°ä¹‹å‰æ’å…¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# collect â€” ç»“æœæ”¶é›†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_collect(args):
    """æ”¶é›†æŒ‡å®šæ¨¡å‹çš„è¯„æµ‹ç»“æœï¼Œå†™å…¥ log/<model_size>/<model_name>/"""
    collect_script = PROJECT_ROOT / "othink_collect.py"
    if not collect_script.exists():
        Log.err(f"ç»“æœæ”¶é›†è„šæœ¬ä¸å­˜åœ¨: {collect_script}")
        sys.exit(1)

    cmd = [
        "uv", "run", "python", str(collect_script),
        "--model", args.model,
        "--model_size", args.model_size,
    ]
    if args.methods:
        cmd.extend(["--methods"] + args.methods)

    rc = run_cmd(cmd)
    if rc == 0:
        Log.ok(f"ç»“æœå·²æ”¶é›†åˆ° log/{args.model_size}/{args.model}/")
    else:
        Log.err(f"ç»“æœæ”¶é›†å¤±è´¥ (rc={rc})")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# report â€” ç”ŸæˆæŠ¥è¡¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cmd_report(args):
    """ç”Ÿæˆè¯„æµ‹æŠ¥è¡¨ (ç»ˆç«¯/Markdown/CSV/LaTeX)"""
    report_script = PROJECT_ROOT / "othink_report.py"
    if not report_script.exists():
        Log.err(f"æŠ¥è¡¨ç”Ÿæˆè„šæœ¬ä¸å­˜åœ¨: {report_script}")
        sys.exit(1)

    cmd = [
        "uv", "run", "python", str(report_script),
        "--format", args.format,
    ]
    if args.models:
        cmd.extend(["--models"] + args.models)
    if args.method:
        cmd.extend(["--method", args.method])
    if args.output:
        cmd.extend(["--output", args.output])
    if args.metric:
        cmd.extend(["--metric", args.metric])

    rc = run_cmd(cmd)
    if rc != 0:
        Log.err(f"æŠ¥è¡¨ç”Ÿæˆå¤±è´¥ (rc={rc})")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åœ¨ build_parser() å‡½æ•°ä¸­ï¼Œeval-all å­å‘½ä»¤ä¹‹åæ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªå­å‘½ä»¤:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"""
    # â”€â”€ collect â”€â”€
    pc = sub.add_parser("collect", help="ğŸ” æ”¶é›†è¯„æµ‹ç»“æœ")
    pc.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    pc.add_argument("--model_size", required=True, help="æ¨¡å‹å¤§å° (0.5B, 1.5B, 7B, 14B)")
    pc.add_argument("--methods", nargs="+", default=None,
                    help="æ–¹æ³•åˆ—è¡¨: standard deer cp-router lcb-standard lcb-deer")
    pc.set_defaults(func=cmd_collect)

    # â”€â”€ report â”€â”€
    pr = sub.add_parser("report", help="ğŸ“Š ç”Ÿæˆè¯„æµ‹æŠ¥è¡¨")
    pr.add_argument("--models", nargs="+", default=None, help="æŒ‡å®šæ¨¡å‹ (é»˜è®¤å…¨éƒ¨)")
    pr.add_argument("--method", default=None, help="åªçœ‹æŸä¸ªæ–¹æ³•")
    pr.add_argument("--format", choices=["terminal", "markdown", "csv", "latex"],
                    default="terminal", help="è¾“å‡ºæ ¼å¼")
    pr.add_argument("--output", default=None, help="è¾“å‡ºåˆ°æ–‡ä»¶")
    pr.add_argument("--metric", choices=["accuracy", "avg_tokens", "both"],
                    default="both", help="æ˜¾ç¤ºæŒ‡æ ‡")
    pr.set_defaults(func=cmd_report)
"""
 
def build_parser():
    p = argparse.ArgumentParser(
        prog="othink_cli",
        description="ğŸ§  OThink-R1 CLI â€” ä¸€ç«™å¼ LLM è¯„æµ‹å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
    ç¤ºä¾‹:
    python othink_cli.py download-data   --datasets all
    python othink_cli.py download-model  --model Qwen/Qwen2.5-7B-Instruct
    python othink_cli.py eval            --model Qwen2.5-0.5B-Instruct --datasets math aime --gpu_ids 0,1
    python othink_cli.py eval-deer       --model Qwen2.5-0.5B-Instruct --datasets math --gpu_ids 0,1
    python othink_cli.py eval-all        --model Qwen2.5-0.5B-Instruct --gpu_ids 0,1,2,3
    python othink_cli.py collect         --model Qwen2.5-0.5B-Instruct --model_size 0.5B
    python othink_cli.py report          --format markdown --output report.md
            """,
    )
    sub = p.add_subparsers(dest="command", help="å­å‘½ä»¤")

    # â”€â”€ download-data â”€â”€
    dd = sub.add_parser("download-data", help="ğŸ“¥ ä¸‹è½½æ•°æ®é›†")
    dd.add_argument("--datasets", nargs="+", required=True,
                    help="æ•°æ®é›†: all, math, aime, asdiv, livecodebench, gsm8k, gpqa")
    dd.add_argument("--hf_mirror", default=DEFAULT_HF_MIRROR, help="HF é•œåƒ")
    dd.set_defaults(func=cmd_download_data)

    # â”€â”€ download-model â”€â”€
    dm = sub.add_parser("download-model", help="ğŸ“¥ ä¸‹è½½æ¨¡å‹")
    dm.add_argument("--model", required=True, help="HF æ¨¡å‹ ID å¦‚ Qwen/Qwen2.5-7B-Instruct")
    dm.add_argument("--name", default=None, help="ä¿å­˜åç§° (é»˜è®¤ä» ID æ¨æ–­)")
    dm.add_argument("--local_path", default=None, help="æœ¬åœ°è·¯å¾„ (åˆ›å»ºè½¯é“¾æ¥)")
    dm.add_argument("--hf_mirror", default=DEFAULT_HF_MIRROR, help="HF é•œåƒ")
    dm.set_defaults(func=cmd_download_model)

    # â”€â”€ eval â”€â”€
    ev = sub.add_parser("eval", help="ğŸ“Š æ ‡å‡†è¯„æµ‹")
    ev.add_argument("--model", required=True, help="æ¨¡å‹åç§° (models/ ä¸‹)")
    ev.add_argument("--datasets", nargs="+", required=True, help="æ•°æ®é›†åˆ—è¡¨")
    ev.add_argument("--gpu_ids", required=True, help="GPU å¡å·, å¦‚ 0,1,2,3")
    ev.add_argument("--temperature", type=float, default=0.9)
    ev.add_argument("--top_p", type=float, default=0.95)
    ev.add_argument("--max_tokens", type=int, default=4096)
    ev.set_defaults(func=cmd_eval)

    # â”€â”€ eval-deer â”€â”€
    ed = sub.add_parser("eval-deer", help="ğŸ¦Œ DEER è¯„æµ‹")
    ed.add_argument("--model", required=True, help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    ed.add_argument("--datasets", nargs="+", required=True, help="æ•°æ®é›†åˆ—è¡¨")
    ed.add_argument("--gpu_ids", required=True, help="GPU å¡å·")
    ed.add_argument("--threshold", type=float, default=0.95, help="DEER é˜ˆå€¼")
    ed.add_argument("--max_len", type=int, default=16384, help="æœ€å¤§é•¿åº¦")
    ed.set_defaults(func=cmd_eval_deer)

    # â”€â”€ eval-cp-router â”€â”€
    ec = sub.add_parser("eval-cp-router", help="ğŸ”€ CP-Router è¯„æµ‹")
    ec.add_argument("--llm_model", required=True, help="LLM æ¨¡å‹")
    ec.add_argument("--lrm_model", default=None, help="LRM æ¨¡å‹ (é»˜è®¤åŒ LLM)")
    ec.add_argument("--datasets", nargs="+", required=True, help="æ•°æ®é›†åˆ—è¡¨")
    ec.add_argument("--gpu_ids", required=True, help="GPU å¡å·")
    ec.add_argument("--tau", type=int, default=1)
    ec.add_argument("--beta", type=float, default=3.0)
    ec.add_argument("--max_samples", type=int, default=0, help="0=å…¨éƒ¨")
    ec.add_argument("--batch_size", type=int, default=8)
    ec.add_argument("--skip_lrm", action="store_true", help="è·³è¿‡ LRM æ¨ç†")
    ec.add_argument("--lrm_max_tokens", type=int, default=512)
    ec.set_defaults(func=cmd_eval_cp_router)

    # â”€â”€ eval-lcb â”€â”€
    el = sub.add_parser("eval-lcb", help="ğŸ’» LiveCodeBench è¯„æµ‹")
    el.add_argument("--model", required=True, help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    el.add_argument("--mode", choices=["standard", "deer"], default="standard")
    el.add_argument("--gpu_ids", required=True, help="GPU å¡å·")
    el.add_argument("--threshold", type=float, default=0.95)
    el.add_argument("--max_problems", type=int, default=0, help="0=å…¨éƒ¨")
    el.set_defaults(func=cmd_eval_lcb)

    # â”€â”€ eval-all â”€â”€
    ea = sub.add_parser("eval-all", help="ğŸš€ ä¸€é”®å…¨é‡è¯„æµ‹")
    ea.add_argument("--model", required=True, help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    ea.add_argument("--gpu_ids", required=True, help="GPU å¡å·")
    ea.add_argument("--methods", default="standard,deer,cp-router,lcb-standard,lcb-deer",
                    help="æ–¹æ³•åˆ—è¡¨, é€—å·åˆ†éš”")
    ea.add_argument("--datasets", nargs="+",
                    default=["math", "aime", "asdiv"], help="æ•°æ®é›†åˆ—è¡¨")
    ea.add_argument("--lrm_model", default=None, help="LRM æ¨¡å‹ (cp-router ç”¨)")
    ea.add_argument("--threshold", type=float, default=0.95)
    ea.add_argument("--max_len", type=int, default=16384)
    ea.set_defaults(func=cmd_eval_all)

    # â”€â”€ collect (æ–°å¢) â”€â”€
    pc = sub.add_parser("collect", help="ğŸ” æ”¶é›†è¯„æµ‹ç»“æœåˆ° log/ ç›®å½•")
    pc.add_argument("--model", required=True, help="æ¨¡å‹åç§° (å¦‚ Qwen2.5-0.5B-Instruct)")
    pc.add_argument("--model_size", required=True, help="æ¨¡å‹å¤§å° (0.5B, 1.5B, 7B, 14B)")
    pc.add_argument("--methods", nargs="+", default=None,
                    help="æ–¹æ³•: standard deer cp-router lcb-standard lcb-deer")
    pc.set_defaults(func=cmd_collect)

    # â”€â”€ report (æ–°å¢) â”€â”€
    pr = sub.add_parser("report", help="ğŸ“Š ç”Ÿæˆè¯„æµ‹æŠ¥è¡¨")
    pr.add_argument("--models", nargs="+", default=None, help="æŒ‡å®šæ¨¡å‹ (é»˜è®¤å…¨éƒ¨)")
    pr.add_argument("--method", default=None,
                    help="åªçœ‹æŸä¸ªæ–¹æ³•: standard / deer / cp-router / lcb-standard / lcb-deer")
    pr.add_argument("--format", choices=["terminal", "markdown", "csv", "latex"],
                    default="terminal", help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: terminal)")
    pr.add_argument("--output", default=None, help="è¾“å‡ºåˆ°æ–‡ä»¶")
    pr.add_argument("--metric", choices=["accuracy", "avg_tokens", "both"],
                    default="both", help="æ˜¾ç¤ºæŒ‡æ ‡ (é»˜è®¤: both)")
    pr.set_defaults(func=cmd_report)

    return p




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = build_parser()
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(0)

    print(r"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘        ğŸ§   OThink-R1 CLI  â€”  LLM Evaluation Suite       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    Log.info(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
    Log.run(f"å‘½ä»¤: {args.command}")
    print()

    try:
        args.func(args)
    except KeyboardInterrupt:
        print()
        Log.warn("ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
        sys.exit(130)
    except Exception as e:
        Log.err(f"å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()