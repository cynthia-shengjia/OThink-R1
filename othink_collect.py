#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OThink-R1 ç»“æœæ”¶é›†å™¨ (othink_collect.py)
=========================================
ä»å„è¯„æµ‹è„šæœ¬çš„åŸå§‹è¾“å‡ºä¸­æå– accuracy å’Œ avg_tokensï¼Œ
ç»Ÿä¸€å†™å…¥:
  log/<model_size>/<model_name>/log.txt        â€” äººç±»å¯è¯»çš„æ±‡æ€»æ—¥å¿—
  log/<model_size>/<model_name>/results.json   â€” ç»“æ„åŒ– JSON (æ¯ä¸ª dataset/method çš„æŒ‡æ ‡)

æ”¾ç½®äºé¡¹ç›®æ ¹ç›®å½•: OThink-R1/othink_collect.py

ç”¨æ³•:
  # æ”¶é›†æŒ‡å®šæ¨¡å‹çš„æ‰€æœ‰ç»“æœ
  python othink_collect.py --model Qwen2.5-0.5B-Instruct --model_size 0.5B

  # æ”¶é›†å¹¶æŒ‡å®šæ–¹æ³•
  python othink_collect.py --model Qwen2.5-0.5B-Instruct --model_size 0.5B --methods standard deer

  # æ”¶é›†å¤šä¸ªæ¨¡å‹
  python othink_collect.py --model Qwen2.5-0.5B-Instruct --model_size 0.5B
  python othink_collect.py --model Qwen2.5-7B-Instruct --model_size 7B
  python othink_collect.py --model DeepSeek-R1-Distill-Qwen-7B --model_size 7B
"""

import argparse
import json
import os
import re
import sys
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any

PROJECT_ROOT = Path(__file__).resolve().parent


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ•°æ®ç»“æ„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class EvalResult:
    """å•æ¬¡è¯„æµ‹çš„ç»“æœ"""
    def __init__(self, method: str, dataset: str):
        self.method = method          # standard / deer / cp-router / lcb-standard / lcb-deer
        self.dataset = dataset        # math / aime / asdiv / gsm8k / gpqa / livecodebench
        self.accuracy: Optional[float] = None
        self.avg_tokens: Optional[float] = None
        self.total_samples: Optional[int] = None
        self.correct_count: Optional[int] = None
        self.extra: Dict[str, Any] = {}  # æ–¹æ³•ç‰¹æœ‰çš„æŒ‡æ ‡ (å¦‚ DEER çš„ threshold, CP-Router çš„ trr)
        self.source_file: Optional[str] = None

    def to_dict(self) -> dict:
        d = {
            "method": self.method,
            "dataset": self.dataset,
            "accuracy": self.accuracy,
            "avg_tokens": self.avg_tokens,
            "total_samples": self.total_samples,
            "correct_count": self.correct_count,
        }
        d.update(self.extra)
        if self.source_file:
            d["source_file"] = self.source_file
        return d


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è§£æå™¨: Standard (OThinkR1Training log)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_standard_logs(model_name: str) -> List[EvalResult]:
    """
    è§£æ OThinkR1Training/log/<dataset>/<size>/test/*.log
    
    æ—¥å¿—æ ¼å¼ (æ¥è‡ª eval_utils.py write_responses):
      ============= Summary =============
      Total cases: 500
      Average tokens: 1234.567
      Correct rate: 0.680
    """
    results = []
    log_base = PROJECT_ROOT / "OThinkR1Training" / "log"
    
    if not log_base.exists():
        return results

    # éå†æ‰€æœ‰æ•°æ®é›†ç›®å½•
    for dataset_dir in log_base.iterdir():
        if not dataset_dir.is_dir():
            continue
        dataset_name = dataset_dir.name  # e.g. "AIME", "MATHBench", "ASDIV"

        # éå†æ‰€æœ‰ size ç›®å½•
        for size_dir in dataset_dir.iterdir():
            if not size_dir.is_dir():
                continue

            # éå† test ç›®å½•ä¸‹çš„ log æ–‡ä»¶
            test_dir = size_dir / "test"
            if not test_dir.exists():
                continue

            for log_file in test_dir.glob("*.log"):
                # æ£€æŸ¥æ˜¯å¦å±äºç›®æ ‡æ¨¡å‹
                # æ–‡ä»¶åæ ¼å¼: models_Qwen2.50.5BInstruct-parallel-1-tmp-0.9-topp-0.95.log
                model_clean = model_name.replace("-", "").replace(".", "")
                if model_clean.lower() not in log_file.name.replace("-", "").replace(".", "").lower():
                    continue

                content = log_file.read_text(encoding="utf-8", errors="ignore")

                r = EvalResult(method="standard", dataset=_normalize_dataset_name(dataset_name))
                r.source_file = str(log_file.relative_to(PROJECT_ROOT))

                # æå– Total cases
                m = re.search(r"Total cases:\s*(\d+)", content)
                if m:
                    r.total_samples = int(m.group(1))

                # æå– Average tokens
                m = re.search(r"Average tokens:\s*([\d.]+)", content)
                if m:
                    r.avg_tokens = float(m.group(1))

                # æå– Correct rate
                m = re.search(r"Correct rate:\s*([\d.]+)", content)
                if m:
                    r.accuracy = float(m.group(1))

                if r.total_samples and r.accuracy is not None:
                    r.correct_count = int(round(r.accuracy * r.total_samples))

                results.append(r)

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è§£æå™¨: DEER
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_deer_results(model_name: str) -> List[EvalResult]:
    """
    è§£æ DEER è¾“å‡º:
    1. baseline/deer/outputs/<model_name>/<dataset>/*.jsonl  â€” åŸå§‹æ¨ç†ç»“æœ
    2. baseline/deer/outputs/<model_name>/<dataset>/*_othink_eval.json â€” OThink verifier è¯„ä¼°
    
    jsonl æ¯è¡Œ: {"question": ..., "generated_responses": [...], "gold_answer": ..., 
                  "too_long": 0/1, "thinking_steps": N, "high_prob": 0/1, ...}
    
    _othink_eval.json: {"accuracy": 0.68, "correct": 34, "total": 50, ...}
    """
    results = []
    deer_outputs = PROJECT_ROOT / "baseline" / "deer" / "outputs"

    if not deer_outputs.exists():
        return results

    # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
    model_dir = deer_outputs / model_name
    if not model_dir.exists():
        # å°è¯•æ¨¡ç³ŠåŒ¹é…
        for d in deer_outputs.iterdir():
            if d.is_dir() and model_name.replace("-", "") in d.name.replace("-", ""):
                model_dir = d
                break
        if not model_dir.exists():
            return results

    for dataset_dir in model_dir.iterdir():
        if not dataset_dir.is_dir():
            continue
        dataset_name = dataset_dir.name

        # ä¼˜å…ˆä½¿ç”¨ _othink_eval.json
        eval_jsons = list(dataset_dir.glob("*_othink_eval.json"))
        jsonl_files = list(dataset_dir.glob("*.jsonl"))

        if eval_jsons:
            # å–æœ€æ–°çš„
            eval_json = sorted(eval_jsons, key=lambda f: f.stat().st_mtime)[-1]
            data = json.loads(eval_json.read_text())

            r = EvalResult(method="deer", dataset=_normalize_dataset_name(dataset_name))
            r.accuracy = data.get("accuracy")
            r.total_samples = data.get("total")
            r.correct_count = data.get("correct")
            r.source_file = str(eval_json.relative_to(PROJECT_ROOT))

            # ä»å¯¹åº”çš„ jsonl æå– avg_tokens
            if jsonl_files:
                jsonl_file = sorted(jsonl_files, key=lambda f: f.stat().st_mtime)[-1]
                r.avg_tokens = _calc_avg_tokens_from_deer_jsonl(jsonl_file)
                r.extra["deer_stats"] = _calc_deer_stats(jsonl_file)

            results.append(r)

        elif jsonl_files:
            # æ²¡æœ‰ eval jsonï¼Œç›´æ¥ä» jsonl è§£æ
            jsonl_file = sorted(jsonl_files, key=lambda f: f.stat().st_mtime)[-1]
            r = EvalResult(method="deer", dataset=_normalize_dataset_name(dataset_name))
            r.source_file = str(jsonl_file.relative_to(PROJECT_ROOT))
            r.avg_tokens = _calc_avg_tokens_from_deer_jsonl(jsonl_file)
            stats = _calc_deer_stats(jsonl_file)
            r.extra["deer_stats"] = stats
            # å¦‚æœ jsonl ä¸­æœ‰ is_correct å­—æ®µ
            if "accuracy" in stats:
                r.accuracy = stats["accuracy"]
                r.total_samples = stats["total"]
                r.correct_count = stats["correct"]
            results.append(r)

    return results


def _calc_avg_tokens_from_deer_jsonl(jsonl_path: Path) -> Optional[float]:
    """ä» DEER jsonl è®¡ç®—å¹³å‡ token æ•° (ç”¨ response å­—ç¬¦æ•°è¿‘ä¼¼, æˆ–ç”¨ thinking_tokens å­—æ®µ)"""
    try:
        total_tokens = 0
        count = 0
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line.strip())
                # ä¼˜å…ˆç”¨ total_tokens å­—æ®µ
                if "total_tokens" in item:
                    total_tokens += item["total_tokens"]
                elif "generated_responses" in item and item["generated_responses"]:
                    # ç”¨ response é•¿åº¦çš„ token è¿‘ä¼¼ (ä¸­æ–‡~1.5å­—/token, è‹±æ–‡~0.75è¯/token)
                    resp = item["generated_responses"][0]
                    # ç²—ç•¥ä¼°è®¡: æŒ‰ç©ºæ ¼åˆ†è¯æ•° * 1.3
                    total_tokens += len(resp.split()) * 1.3
                count += 1
        return total_tokens / count if count > 0 else None
    except Exception:
        return None


def _calc_deer_stats(jsonl_path: Path) -> dict:
    """ä» DEER jsonl è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
    stats = {"total": 0, "correct": 0, "too_long": 0, "high_prob": 0, "regular_end": 0}
    try:
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line.strip())
                stats["total"] += 1
                if item.get("is_correct"):
                    stats["correct"] += 1
                if item.get("too_long"):
                    stats["too_long"] += 1
                if item.get("high_prob"):
                    stats["high_prob"] += 1
                if item.get("regular_end"):
                    stats["regular_end"] += 1
        if stats["total"] > 0:
            stats["accuracy"] = stats["correct"] / stats["total"]
    except Exception:
        pass
    return stats


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è§£æå™¨: CP-Router
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_cp_router_results(model_name: str) -> List[EvalResult]:
    """
    è§£æ baseline/cp-router/results/test_<dataset>_*.json
    
    JSON æ ¼å¼:
    {
      "dataset": "asdiv",
      "model": "...",
      "alpha_star": 0.15,
      "apss": 1.23,
      "llm_acc": 0.65,
      "router_acc": 0.70,
      "trr": 0.45,
      "u_token": 0.09,
      "llm_count": 14,
      "lrm_count": 6,
      ...
    }
    """
    results = []
    cp_results_dir = PROJECT_ROOT / "baseline" / "cp-router" / "results"

    if not cp_results_dir.exists():
        return results

    for json_file in cp_results_dir.glob("test_*.json"):
        try:
            data = json.loads(json_file.read_text())
        except Exception:
            continue

        # æ£€æŸ¥æ˜¯å¦å±äºç›®æ ‡æ¨¡å‹
        model_field = data.get("model", "")
        if model_name not in model_field and model_name.replace("-", "") not in model_field.replace("-", ""):
            continue

        dataset = data.get("dataset", "unknown")
        r = EvalResult(method="cp-router", dataset=_normalize_dataset_name(dataset))
        r.accuracy = data.get("router_acc")
        r.total_samples = data.get("n_test")
        r.source_file = str(json_file.relative_to(PROJECT_ROOT))

        # CP-Router ç‰¹æœ‰æŒ‡æ ‡
        r.extra["llm_acc"] = data.get("llm_acc")
        r.extra["trr"] = data.get("trr")
        r.extra["u_token"] = data.get("u_token")
        r.extra["apss"] = data.get("apss")
        r.extra["alpha_star"] = data.get("alpha_star")
        r.extra["llm_count"] = data.get("llm_count")
        r.extra["lrm_count"] = data.get("lrm_count")

        results.append(r)

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è§£æå™¨: LiveCodeBench
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_lcb_results(model_name: str) -> List[EvalResult]:
    """
    è§£æ results/lcb/<model_name>/standard/metrics.json
    å’Œ    results/lcb/<model_name>/deer_t*/metrics.json
    
    metrics.json æ ¼å¼ (æ¥è‡ª lcb_eval.py):
    {
      "pass@1": 0.123,
      "easy_pass@1": 0.456,
      "medium_pass@1": 0.234,
      "hard_pass@1": 0.012,
      ...
    }
    """
    results = []
    lcb_base = PROJECT_ROOT / "results" / "lcb"

    if not lcb_base.exists():
        return results

    model_dir = lcb_base / model_name
    if not model_dir.exists():
        # æ¨¡ç³ŠåŒ¹é…
        for d in lcb_base.iterdir():
            if d.is_dir() and model_name.replace("-", "") in d.name.replace("-", ""):
                model_dir = d
                break
        if not model_dir.exists():
            return results

    for sub_dir in model_dir.iterdir():
        if not sub_dir.is_dir():
            continue

        metrics_file = sub_dir / "metrics.json"
        if not metrics_file.exists():
            continue

        try:
            data = json.loads(metrics_file.read_text())
        except Exception:
            continue

        # åˆ¤æ–­æ˜¯ standard è¿˜æ˜¯ deer
        if "standard" in sub_dir.name:
            method = "lcb-standard"
        elif "deer" in sub_dir.name:
            method = "lcb-deer"
        else:
            method = f"lcb-{sub_dir.name}"

        r = EvalResult(method=method, dataset="livecodebench")
        r.accuracy = data.get("pass@1")
        r.source_file = str(metrics_file.relative_to(PROJECT_ROOT))

        # LCB ç‰¹æœ‰æŒ‡æ ‡
        for key in ["easy_pass@1", "medium_pass@1", "hard_pass@1"]:
            if key in data:
                r.extra[key] = data[key]

        # ä» generation_results.json æˆ– deer_results.json æå– token ä¿¡æ¯
        gen_file = sub_dir / "generation_results.json"
        deer_file = sub_dir / "deer_results.json"
        result_file = gen_file if gen_file.exists() else (deer_file if deer_file.exists() else None)

        if result_file:
            try:
                gen_data = json.loads(result_file.read_text())
                total_tokens = 0
                count = 0
                for item in gen_data:
                    if "output_list" in item and item["output_list"]:
                        total_tokens += len(item["output_list"][0].split()) * 1.3
                        count += 1
                    if "deer_rounds" in item:
                        r.extra.setdefault("deer_stats", {})
                        r.extra["deer_stats"]["avg_rounds"] = r.extra["deer_stats"].get("avg_rounds", 0) + item["deer_rounds"]
                if count > 0:
                    r.avg_tokens = total_tokens / count
                    if "deer_stats" in r.extra:
                        r.extra["deer_stats"]["avg_rounds"] /= count
                r.total_samples = count
            except Exception:
                pass

        results.append(r)

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¾…åŠ©å‡½æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _normalize_dataset_name(name: str) -> str:
    """ç»Ÿä¸€æ•°æ®é›†åç§°"""
    mapping = {
        "MATHBench": "math",
        "MATH": "math",
        "math_hf": "math",
        "AIME": "aime",
        "aime_hf": "aime",
        "ASDIV": "asdiv",
        "asdiv_hf": "asdiv",
        "GSM8K": "gsm8k",
        "gsm8k": "gsm8k",
        "GPQA": "gpqa",
        "gpqa": "gpqa",
        "math_test10": "math(test10)",
    }
    return mapping.get(name, name.lower())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»æ”¶é›†é€»è¾‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_all(model_name: str, model_size: str,
                methods: Optional[List[str]] = None) -> List[EvalResult]:
    """
    æ”¶é›†æŒ‡å®šæ¨¡å‹çš„æ‰€æœ‰è¯„æµ‹ç»“æœã€‚
    
    Args:
        model_name: æ¨¡å‹åç§° (å¦‚ Qwen2.5-0.5B-Instruct)
        model_size: æ¨¡å‹å¤§å° (å¦‚ 0.5B, 7B, 14B)
        methods: è¦æ”¶é›†çš„æ–¹æ³•åˆ—è¡¨, None=å…¨éƒ¨
    
    Returns:
        æ‰€æœ‰è¯„æµ‹ç»“æœåˆ—è¡¨
    """
    all_results = []
    
    collectors = {
        "standard": parse_standard_logs,
        "deer": parse_deer_results,
        "cp-router": parse_cp_router_results,
        "lcb-standard": parse_lcb_results,
        "lcb-deer": parse_lcb_results,
    }

    target_methods = methods or list(collectors.keys())

    for method in target_methods:
        if method in ("lcb-standard", "lcb-deer"):
            # LCB è§£æå™¨ç»Ÿä¸€å¤„ç†
            if method == "lcb-deer" and "lcb-standard" in target_methods:
                continue  # é¿å…é‡å¤è°ƒç”¨
            parser = collectors[method]
        else:
            parser = collectors.get(method)

        if parser is None:
            print(f"âš ï¸  æœªçŸ¥æ–¹æ³•: {method}, è·³è¿‡")
            continue

        try:
            results = parser(model_name)
            if methods:
                results = [r for r in results if r.method in target_methods]
            all_results.extend(results)
        except Exception as e:
            print(f"âš ï¸  è§£æ {method} ç»“æœå¤±è´¥: {e}")

    return all_results


def write_outputs(model_name: str, model_size: str, results: List[EvalResult]):
    """
    å°†ç»“æœå†™å…¥:
      log/<model_size>/<model_name>/log.txt
      log/<model_size>/<model_name>/results.json
    """
    output_dir = PROJECT_ROOT / "log" / model_size / model_name
    output_dir.mkdir(parents=True, exist_ok=True)

    log_path = output_dir / "log.txt"
    json_path = output_dir / "results.json"

    # â”€â”€ å†™ log.txt â”€â”€
    lines = []
    lines.append("=" * 70)
    lines.append(f"  OThink-R1 è¯„æµ‹ç»“æœæ±‡æ€»")
    lines.append(f"  æ¨¡å‹: {model_name}")
    lines.append(f"  å¤§å°: {model_size}")
    lines.append(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("=" * 70)
    lines.append("")

    # æŒ‰æ–¹æ³•åˆ†ç»„
    by_method: Dict[str, List[EvalResult]] = {}
    for r in results:
        by_method.setdefault(r.method, []).append(r)

    for method, method_results in sorted(by_method.items()):
        lines.append(f"â”€â”€ {method.upper()} {'â”€' * (60 - len(method))}")
        lines.append(f"  {'æ•°æ®é›†':<15} {'Accuracy':>10} {'Avg Tokens':>12} {'Samples':>10}")
        lines.append(f"  {'-'*15} {'-'*10} {'-'*12} {'-'*10}")

        for r in sorted(method_results, key=lambda x: x.dataset):
            acc_str = f"{r.accuracy:.4f}" if r.accuracy is not None else "N/A"
            tok_str = f"{r.avg_tokens:.1f}" if r.avg_tokens is not None else "N/A"
            sam_str = str(r.total_samples) if r.total_samples is not None else "N/A"
            lines.append(f"  {r.dataset:<15} {acc_str:>10} {tok_str:>12} {sam_str:>10}")

            # é¢å¤–æŒ‡æ ‡
            if r.extra:
                for k, v in r.extra.items():
                    if k == "deer_stats":
                        continue
                    if isinstance(v, float):
                        lines.append(f"    {k}: {v:.4f}")
                    elif v is not None:
                        lines.append(f"    {k}: {v}")

        lines.append("")

    # æ€»ç»“
    lines.append("=" * 70)
    lines.append(f"  å…± {len(results)} æ¡è¯„æµ‹è®°å½•")
    lines.append("=" * 70)

    log_content = "\n".join(lines)
    log_path.write_text(log_content, encoding="utf-8")
    print(f"âœ… æ—¥å¿—å·²å†™å…¥: {log_path}")

    # â”€â”€ å†™ results.json â”€â”€
    json_data = {
        "model_name": model_name,
        "model_size": model_size,
        "collected_at": datetime.now().isoformat(),
        "results": [r.to_dict() for r in results],
    }
    json_path.write_text(json.dumps(json_data, indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"âœ… JSON å·²å†™å…¥: {json_path}")

    # æ‰“å°åˆ°ç»ˆç«¯
    print()
    print(log_content)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser(
        description="ğŸ” OThink-R1 ç»“æœæ”¶é›†å™¨ â€” ä»å„è¯„æµ‹è¾“å‡ºä¸­æå– accuracy + tokens",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python othink_collect.py --model Qwen2.5-0.5B-Instruct --model_size 0.5B
  python othink_collect.py --model Qwen2.5-7B-Instruct --model_size 7B --methods standard deer
  python othink_collect.py --model DeepSeek-R1-Distill-Qwen-7B --model_size 7B --methods deer cp-router
        """,
    )
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§° (å¦‚ Qwen2.5-0.5B-Instruct)")
    parser.add_argument("--model_size", required=True, help="æ¨¡å‹å¤§å° (å¦‚ 0.5B, 1.5B, 7B, 14B)")
    parser.add_argument("--methods", nargs="+", default=None,
                        help="è¦æ”¶é›†çš„æ–¹æ³•: standard deer cp-router lcb-standard lcb-deer (é»˜è®¤å…¨éƒ¨)")

    args = parser.parse_args()

    print(f"ğŸ” æ”¶é›†æ¨¡å‹ [{args.model}] ({args.model_size}) çš„è¯„æµ‹ç»“æœ...")
    print()

    results = collect_all(args.model, args.model_size, args.methods)

    if not results:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è¯„æµ‹ç»“æœ!")
        print("   è¯·ç¡®è®¤:")
        print(f"   - æ ‡å‡†è¯„æµ‹æ—¥å¿—: OThinkR1Training/log/*/")
        print(f"   - DEER è¾“å‡º: baseline/deer/outputs/{args.model}/")
        print(f"   - CP-Router ç»“æœ: baseline/cp-router/results/")
        print(f"   - LCB ç»“æœ: results/lcb/{args.model}/")
        sys.exit(1)

    write_outputs(args.model, args.model_size, results)


if __name__ == "__main__":
    main()